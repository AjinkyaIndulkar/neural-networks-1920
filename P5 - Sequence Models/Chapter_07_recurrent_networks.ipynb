{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author contributions\n",
    "Please fill out for each of the following parts who contributed to what:\n",
    "- Conceived ideas: \n",
    "- Performed math exercises: \n",
    "- Performed programming exercises:\n",
    "- Contributed to the overall final assignment: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7\n",
    "## Recurrent neural networks\n",
    "\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Brightspace before the deadline (see Brightspace).\n",
    "\n",
    "Learning goals:\n",
    "1. Get familiar with recurrent hidden units\n",
    "1. Implement a simple RNN (Elman network) in PyTorch\n",
    "1. Implement an LSTM-based neural network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1  (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a recurrent neural network with one input unit $x$, one sigmoid recurrent hidden unit $h$, and one linear output unit $y$. The values of $x$ are given for 3 time points in `x_t`. As this is a very small RNN, $W^i$, $W^h$ and $W^o$ are given as the scalar values `w_i`, `w_h` and `w_o` respectively. The hidden unit has an added bias `h_bias`. The hidden unit state is initialized with `0.0`. The only 'value-manipulating' activation function in this network is the sigmoid activation $\\sigma(\\cdot)$ on the hidden unit. \n",
    "\n",
    "1. Write down the forward pass of this network for a specific time point $t$. \n",
    "1. What is the value of the hidden state $h$ after processing the last input `x_t[2]`? \n",
    "1. What is the output `y` of the network after processing the last input `x_t[2]`? \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "h_t &=& \\\\ \n",
    "y_t &=&  \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "For 1.2 and 1.3, you can either compute the solution by hand (show clearly how you arrived there, 3 decimal points) or write code to find the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs over times 0, 1, 2:\n",
    "x_t = [9.0, 4.0, -2.0]\n",
    "\n",
    "# weights and bias terms: \n",
    "w_i = 0.5\n",
    "w_h = -1.0\n",
    "w_o = -0.7\n",
    "h_bias = -1.0\n",
    "y_bias = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LaTex or code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code introduction\n",
    "\n",
    "We will apply two recurrent neural networks to learn a dynamic variant of the *adding problem*. First, run the next cell and inspect the output. \n",
    "\n",
    "There is a stream of inputs to the network, two at each time step. The first input unit will receive a series of decimal numbers in the interval $[-1,1]$. The second input unit will receive the numbers $0$, $-1$, or $1$. The target is the sum of the preceding two decimal numbers that came together with the number $1$ (called the marker, `x` in the generated output), and it should be produced whenever a marker has been seen. In the beginning until two of these markers have been seen, the output will stay 0. \n",
    "\n",
    "\n",
    "Below you will find two functions: \n",
    "1. `create_addition_data`: Generates sequential training data sets `X` and `T` for the dynamic *adding problem*, returns numpy array.\n",
    "1. `MyDataset`: a custom PyTorch dataset that makes sure dimensions are as PyTorch likes them, and can return individual samples the way PyTorch wants them.\n",
    "\n",
    "Note, the data are represented in a dictionary called `data`. To access the training, validation, and testing data, you can call `data[\"train\"]`, `data[\"valid]`, and `data[\"test\"]` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_addition_data(n_samples=3000):\n",
    "    # This is a dynamic variant of the adding problem. \n",
    "    \n",
    "    # Random numbers in [-1.0,1.0]): \n",
    "    X1 = np.random.uniform(low=-1.0, high=1.0, size=(n_samples,) )   \n",
    "    \n",
    "    # Random markers [-1.0, 0.0, 1.0] (1.0 marks the numbers that should be added):\n",
    "    X2 = np.random.choice([-1.0, 0.0, 1.0], size=(n_samples,), p=[0.25, 0.25, 0.5])\n",
    "    \n",
    "    # Combine\n",
    "    X = np.vstack((X1, X2)).T.astype(\"float32\")\n",
    "\n",
    "    # Create targets\n",
    "    T = np.zeros((n_samples, 1)).astype(\"float32\")\n",
    "\n",
    "    # Get indices of 1.0\n",
    "    markers = np.nonzero(X2 == 1.0)[0]\n",
    "    \n",
    "    # Generate data\n",
    "    mem = X1[markers[0]]\n",
    "    for mi, marker in enumerate(markers[1:]):\n",
    "        T[marker] = mem + X1[marker]\n",
    "        mem = X1[marker]\n",
    "                \n",
    "    return X, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long as the markers x are sparse\n",
    "X, T = create_addition_data(n_samples=100)\n",
    "\n",
    "# Print some data\n",
    "print(\"Data for the adding problem (x marks 1.0):\")\n",
    "for t in range(X.shape[0]):\n",
    "    print(\"Time: {:03d} \\t x: ({:+.3f} , {}) \\t t: {:+.3f} \".format(\n",
    "        t, X[t,0], 'x' if X[t,1] == 1.0 else ' ', T[t,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make PyTorch dataset\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X, T):\n",
    "        self.X = torch.from_numpy(X).type(torch.FloatTensor) # [n_examples, n_samples, n_features]\n",
    "        self.T = torch.from_numpy(T).type(torch.FloatTensor) # [n_examples, n_samples]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index, :, :], self.T[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 9\n",
    "n_samples = 3000\n",
    "data = {}\n",
    "\n",
    "# Define training data\n",
    "X = np.zeros((n_examples, n_samples, 2))\n",
    "T = np.zeros((n_examples, n_samples, 1))\n",
    "for i_example in range(n_examples):\n",
    "    X[i_example, :, :], T[i_example, :] = create_addition_data(n_samples)\n",
    "data[\"train\"] = torch.utils.data.DataLoader(MyDataset(X, T), batch_size=3)\n",
    "\n",
    "# Define validation data\n",
    "X = np.zeros((n_examples, n_samples, 2))\n",
    "T = np.zeros((n_examples, n_samples, 1))\n",
    "for i_example in range(n_examples):\n",
    "    X[i_example, :, :], T[i_example, :] = create_addition_data(n_samples)\n",
    "data[\"valid\"] = torch.utils.data.DataLoader(MyDataset(X, T), batch_size=3)\n",
    "\n",
    "# Define test data\n",
    "X = np.zeros((n_examples, n_samples, 2))\n",
    "T = np.zeros((n_examples, n_samples, 1))\n",
    "for i_example in range(n_examples):\n",
    "    X[i_example, :, :], T[i_example, :] = create_addition_data(n_samples)\n",
    "data[\"test\"] = torch.utils.data.DataLoader(MyDataset(X, T), batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: training a network  (0.5 points)\n",
    "\n",
    "We neede a function to train a `model`. This function `train_model(model, data, optimizer, criterion, n_epochs)` should do the following: \n",
    "\n",
    "1. Loop `n_epochs` times over the dataset, and loop over minibatches\n",
    "1. Train the model on the training data and save the loss per epoch\n",
    "1. Validate the model on the validation data and save the loss per epoch\n",
    "1. The function should return the trained model and the losses\n",
    "\n",
    "Note: this function is quite similar again as the function your wrote wor the MLP and CNN. The only difference is that we do not need to compute an accuracy, as we are performing regression here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Testing a network  (1.5 points)\n",
    "\n",
    "We neede a function to test a trained `model`. This function `test_model(model, data)` should do the following: \n",
    "\n",
    "1. Let `model` predict outputs on the testing data. For this, iterate through test data `data[\"test]` and pass each sample through `model`. \n",
    "1. Save the model output as well as the target output\n",
    "1. The function should return the predicted and target outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Simple RNN  (3 points)\n",
    "\n",
    "We first implement a simple recurrent architecture (a simple [Elman network](http://mnemstudio.org/neural-networks-elman.htm)). \n",
    "\n",
    "1. First implement the linear layers `l1` and `l2`. They should lead from `n_input` input units over `n_hidden` hidden units to `n_out` output units.\n",
    "1. Add a recurrent linear weight layer `hr`. These are weights that self-connect to the hidden units. The input will be the values of the `n_hidden` hidden units, and they should project back to the `n_hidden` hidden units. \n",
    "1. A forward pass will update the hidden state with the inputs and the recurrent layer weights, and produce the output from the hidden unit. Specifically you should do the following: \n",
    "    2. If we are at the first time point, the hidden state should be set to the input passed through `l1` and `tanh` activations.\n",
    "    2. If the hidden state has information from previous time points: a) Pass the input through `l1`. b) Pass the hidden state through the recurrent weight layer `hr`. c) The sum of a) and b) should be passed through the `tanh` activation. d) The result should be the new hidden state (used for the next time point).\n",
    "    2. Finally pass the hidden state through layer `l2`. This produces the output `y` for that time point.\n",
    "1. The forward pass will receive data `x` with shape [batch_size, time_points, features]. So within the forward pass, you will have to loop over time points, performing the steps as descibed above. The output of the forward pass is then output `y` with shape [batch_size, time_points, 1].\n",
    "\n",
    "Note: this exercise could also be done with nn.RNN(). However, we want you to understand what a RNN is doing, so we want you to use nn.Linear instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Setup and run (1 point)\n",
    "\n",
    "Try your simple `RNN` with the dynamic addition task. \n",
    "\n",
    "1. Define the model. `RNN` should have **2 hidden units**.\n",
    "1. Define the loss as the Mean Squared Error loss, and use an Adam optimizer.\n",
    "1. Train your model for several epochs on the data with `train_model`.\n",
    "1. Plot the train and validation losses. \n",
    "1. Test the trained model with `test_model` \n",
    "1. Plot at least one target time series together with the predicted time series\n",
    "\n",
    "Based on the losses and predictions, what would your conclusion be? Did the simple RNN learn the task? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: LSTM RNN (2 points)\n",
    "\n",
    "Long-Short Term Memory (LSTM) units have more [powerful functionality](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), such as selective forgetting, and they are able to keep track of long-term dependencies. This might be useful for the adding task. \n",
    "\n",
    "Implement the `LSTM` model:\n",
    "\n",
    "1. `lstm` should be an `LSTM` layer leading from the `n_input` inputs to the `n_hidden` hidden units.\n",
    "1. `fc` should be a fully-connected (linear) layer leading from the hidden units (output of `lstm`) to the `n_out` output units. \n",
    "1. The network does not make use of further activation functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 7: Setup and run (1 point)\n",
    "\n",
    "Try your `LSTM` model with the dynamic addition task. \n",
    "\n",
    "1. Define the model. `LSTM` should have **2 hidden units**.\n",
    "1. Define the loss as the Mean Squared Error loss, and use an Adam optimizer.\n",
    "1. Train your model for several epochs on the data with `train_model`.\n",
    "1. Plot the train and validation losses. \n",
    "1. Test the trained model with `test_model` \n",
    "1. Plot at least one target time series together with the predicted time series\n",
    "\n",
    "Did the LSTM network capture the task better? Did any of the two capture the task perfectly? Or are the two networks on par? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
